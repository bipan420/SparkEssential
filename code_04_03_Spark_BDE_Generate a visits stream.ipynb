{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44b07dad-e49c-4016-89e0-1458e4667e77",
   "metadata": {},
   "source": [
    "### Create a stream of website visits records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6eba7cc-5087-426c-9d63-40891b5021ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kafka.vendor.six.moves'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KafkaProducer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spark/lib/python3.12/site-packages/kafka/__init__.py:23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madmin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KafkaAdminClient\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient_async\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KafkaClient\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconsumer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KafkaConsumer\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconsumer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msubscription_state\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConsumerRebalanceListener\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproducer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KafkaProducer\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spark/lib/python3.12/site-packages/kafka/consumer/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m absolute_import\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconsumer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroup\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KafkaConsumer\n\u001b[1;32m      5\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKafkaConsumer\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spark/lib/python3.12/site-packages/kafka/consumer/group.py:13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvendor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m six\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient_async\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KafkaClient, selectors\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconsumer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfetcher\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Fetcher\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconsumer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msubscription_state\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SubscriptionState\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcoordinator\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconsumer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConsumerCoordinator\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spark/lib/python3.12/site-packages/kafka/consumer/fetcher.py:19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfetch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FetchRequest\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moffset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     OffsetRequest, OffsetResetStrategy, UNKNOWN_OFFSET\n\u001b[1;32m     18\u001b[0m )\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecord\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MemoryRecords\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserializer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Deserializer\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstructs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TopicPartition, OffsetAndTimestamp\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spark/lib/python3.12/site-packages/kafka/record/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecord\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory_records\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MemoryRecords, MemoryRecordsBuilder\n\u001b[1;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMemoryRecords\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMemoryRecordsBuilder\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spark/lib/python3.12/site-packages/kafka/record/memory_records.py:27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CorruptRecordException\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecord\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ABCRecords\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecord\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy_records\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LegacyRecordBatch, LegacyRecordBatchBuilder\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecord\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdefault_records\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DefaultRecordBatch, DefaultRecordBatchBuilder\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMemoryRecords\u001b[39;00m(ABCRecords):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spark/lib/python3.12/site-packages/kafka/record/legacy_records.py:50\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecord\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ABCRecord, ABCRecordBatch, ABCRecordBatchBuilder\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecord\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m calc_crc32\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcodec\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     51\u001b[0m     gzip_encode, snappy_encode, lz4_encode, lz4_encode_old_kafka,\n\u001b[1;32m     52\u001b[0m     gzip_decode, snappy_decode, lz4_decode, lz4_decode_old_kafka,\n\u001b[1;32m     53\u001b[0m )\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcodec\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcodecs\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CorruptRecordException, UnsupportedCodecError\n",
      "File \u001b[0;32m/opt/anaconda3/envs/spark/lib/python3.12/site-packages/kafka/codec.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstruct\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvendor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m six\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvendor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msix\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmoves\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mrange\u001b[39m\n\u001b[1;32m     11\u001b[0m _XERIAL_V1_HEADER \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m126\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m _XERIAL_V1_FORMAT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbccccccBii\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kafka.vendor.six.moves'"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "\n",
    "visits_topic=\"spark.streaming.website.visits\"\n",
    "\n",
    "def string_serializer(value):\n",
    "    return value.encode('utf-8')\n",
    "    \n",
    "visits_producer=KafkaProducer( \n",
    "                bootstrap_servers=[\"localhost:9092\"],\n",
    "                key_serializer=string_serializer,\n",
    "                value_serializer=string_serializer\n",
    "                )\n",
    "\n",
    "countries = [\"USA\",\"India\",\"Brazil\",\"Australia\",\"Russia\"]\n",
    "last_actions = [\"Catalog\",\"FAQ\",\"Order\",\"ShoppingCart\"]\n",
    "\n",
    "#Generate 100 sample visit records\n",
    "for i in range(1,100):\n",
    "\n",
    "    #Create a json string with generated data\n",
    "    json_record={}\n",
    "    json_record[\"country\"]=countries[random.randint(0,4)]\n",
    "    json_record[\"last_action\"]=last_actions[random.randint(0,3)]\n",
    "    json_record[\"visit_date\"]=datetime.datetime.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    json_record[\"duration\"]=random.randint(1,20)\n",
    "\n",
    "    #Use country as Key. Each country will go through the same partition, \n",
    "    #hence updates to a given country can be handled sequencially\n",
    "    kafka_key=json_record[\"country\"]\n",
    "    kafka_value = json.dumps(json_record)\n",
    "    print(kafka_value)\n",
    "    visits_producer.send(visits_topic, key=kafka_key,value=kafka_value)\n",
    "\n",
    "    #sleep for 1-3 seconds\n",
    "    time.sleep(random.randint(1,3))\n",
    "\n",
    "visits_producer.flush()\n",
    "visits_producer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f16727-11bc-497a-9946-2d282f51b6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consumer test - to test if topic data is published correctly\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "def string_deserializer(value):\n",
    "    return value.decode('utf-8')\n",
    "    \n",
    "visits_consumer=KafkaConsumer(\n",
    "                bootstrap_servers=[\"localhost:9092\"],\n",
    "                value_deserializer=string_deserializer,\n",
    "                auto_offset_reset=\"latest\"\n",
    "                )\n",
    "\n",
    "visits_consumer.subscribe(visits_topic)\n",
    "\n",
    "messages = visits_consumer.poll()\n",
    "print(\"Total messages :\", len(messages))\n",
    "for message in messages:\n",
    "    print(\"%d:%d: vs=%s\" % (message.partition, message.offset, message.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422315d8-b980-47af-bcfd-733cdb89d09c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212198c8-1944-4fae-b67d-f84375672ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
